# -*- coding: utf-8 -*-
"""Python: Fraud Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qwhPsnBdY6ykEXK_YZgw6cd_SGhmYAS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Importons la base de données
df = pd.read_csv('fraud_dataset_example.csv')
df

# Vérifions la présence de valeurs manquantes 
df.isna().sum()

"""Nous pouvons remarquer que notre base de données ne présente pas de valeurs manquantes."""

# Informons nous sur notre Base 
df.info()

"""Nous avons 5 variables de type float, 3 variables de type int et 3 variables de type object qui ne sont pas numériques."""

# La liste de nos variable qualitatives
object_cols = df.dtypes == 'object'
data_cols = list(object_cols[object_cols].index)
data_cols

# La liste de nos variable numériques
numeric_cols = (df.dtypes != 'object')
data_num = list(numeric_cols[numeric_cols].index)
data_num

# Représentation graphique de nos différentes catégories de variable
sns.countplot(x=df.dtypes.map(str))

df.type= df.type.str.strip().str.lower()

# Utilisons la fonction describe pour toujours décrire notre base de données.
df.describe()

"""Tout d'abord, les statistiques descriptives pour les variables quantitatives (amount, oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest) donnent une idée des tendances et des répartitions des valeurs dans la base de données. Par exemple, la moyenne de la variable "amount" est d'environ 180 000, tandis que la médiane est de 74 752, ce qui suggère une distribution biaisée vers des montants plus élevés. Les autres variables ont également des moyennes supérieures à leurs médianes, ce qui indique également des distributions biaisées vers des valeurs plus élevées."""

# Représentation graphique de nos différentes variables numériques
sns.countplot(x=df.type)

f = plt.figure(figsize=(17,5))

ax = f.add_subplot(121)
sns.countplot(x=df.isFraud)

ax = f.add_subplot(122)
sns.countplot(x=df.isFlaggedFraud)

"""Comme cela est le cas le plus souvent avec cette catégorie de donnée, nous pouvons constater que notre base de données est déséquilibrée avec une écrasante majorité de cas de non fraude."""

# Créer un boxplot pour la variable "oldbalanceOrg"
df.boxplot(column="oldbalanceOrg")

"""Enfin, les graphiques (histogramme et boxplot) fournissent une visualisation des distributions des variables quantitatives "amount" et "oldbalanceOrg". Le boxplot pour "oldbalanceOrg" montre la présence de valeurs aberrantes (outliers) qui sont beaucoup plus élevées que la moyenne, ce qui pourrait indiquer une activité frauduleuse. L'histogramme pour la variable "amount" montre une distribution biaisée, avec un pic à environ 0,5 million et une longue queue de valeurs plus élevées, ce qui pourrait également indiquer une activité frauduleuse.

Dans l'ensemble, l'analyse statistique de cette base de données peut aider à identifier des modèles et des anomalies potentielles qui peuvent indiquer une activité de fraude. Les résultats doivent être interprétés avec prudence et en combinaison avec d'autres analyses et enquêtes pour confirmer ou infirmer la présence de fraudes.
"""

# Visualisons la corrélation entre nos variables
sns.heatmap(df.corr(), annot=True)

"""Nous pouvons remarquer d'après le graphique ci dessus, que les variables oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest sont fortements corrélées. """

df.hist(figsize=(14,14))
plt.show()

"""Ces graphiques ci dessus, nous donne une idée de la repartion de nos différentes variables."""

#####Decision Tree########
# Sélectionner les caractéristiques pertinentes pour l'arbre de décision
features = ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']

# Diviser les données en ensemble d'entraînement et ensemble de test
X_train, X_test, y_train, y_test = train_test_split(df[features], df['isFraud'], test_size=0.2, random_state=42)

from sklearn.preprocessing import OneHotEncoder

# Convertir la colonne 'type' en variables numériques
enc = OneHotEncoder(handle_unknown='ignore')
X_train_enc = enc.fit_transform(X_train[['type']])
X_test_enc = enc.transform(X_test[['type']])

# Concaténer les variables numériques avec le reste des variables
X_train_final = np.concatenate((X_train[['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']].values, X_train_enc.toarray()), axis=1)
X_test_final = np.concatenate((X_test[['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']].values, X_test_enc.toarray()), axis=1)

# Créer et entraîner l'arbre de décision
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train_final, y_train)

# Prédire les classes sur l'ensemble de test
y_pred = clf.predict(X_test_final)

# Évaluer la performance de l'arbre de décision
from sklearn.metrics import confusion_matrix
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Confusion matrix:\n', confusion_matrix(y_test, y_pred))
from sklearn.metrics import classification_report
print('Classification report:\n', classification_report(y_test, y_pred))

"""La précision de l'arbre de décision était évaluée à l'aide de la métrique F1-score. Le F1-score est une mesure de la précision de la classification qui tient compte à la fois de la précision et du rappel.

En termes d'approche financière, l'utilisation de l'arbre de décision pour la détection de fraudes bancaires permet de détecter les caractéristiques les plus importantes pour la détection de fraude. Par exemple, dans le code fourni, les caractéristiques les plus importantes étaient le montant de la transaction, le solde du compte de l'expéditeur, le solde du compte du bénéficiaire et le type de transaction.

La précision pour la classe 0 est de 1.00, ce qui signifie que tous les échantillons prédits comme non frauduleux (classe 0) étaient effectivement non frauduleux.

La précision pour la classe 1 est de 0.44, ce qui signifie que seulement 44% des échantillons prédits comme frauduleux (classe 1) étaient effectivement frauduleux.

Le rappel pour la classe 0 est également de 1.00, ce qui signifie que tous les échantillons réels non frauduleux ont été correctement identifiés comme non frauduleux.

Le rappel pour la classe 1 est de 0.60, ce qui signifie que seulement 60% des échantillons réels frauduleux ont été correctement identifiés comme frauduleux.

Le f1-score pour la classe 0 est de 1.00, ce qui signifie une harmonique entre la précision et le rappel égale à 1.

Le f1-score pour la classe 1 est de 0.51, ce qui signifie une harmonique entre la précision et le rappel faible.

Dans l'ensemble, cela indique que l'algorithme est très précis pour la classe non frauduleuse, mais pas très performant pour détecter la classe frauduleuse. Ceci est également reflété dans la confusion matrix qui montre un grand nombre de faux négatifs (8) pour la classe frauduleuse.
"""



#######KNN########
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Charger les données dans une dataframe pandas
df = pd.read_csv("fraud_dataset_example.csv")

# Sélectionner les caractéristiques pertinentes pour KNN
features = ['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

# Diviser les données en ensemble d'entraînement et ensemble de test
X_train, X_test, y_train, y_test = train_test_split(df[features], df['isFraud'], test_size=0.2, random_state=42)

# Diviser les données en ensemble d'entraînement et ensemble de test
X_train, X_test, y_train, y_test = train_test_split(df[features], df['isFraud'], test_size=0.2, random_state=42)

# Créer et entraîner le modèle KNN
k = 5  # nombre de voisins à considérer
clf = KNeighborsClassifier(n_neighbors=k)
clf.fit(X_train_enc, y_train)

# Faire des prédictions sur l'ensemble de test
y_pred = clf.predict(X_test_enc)

# Évaluer les performances du modèle
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

conf_mat = confusion_matrix(y_test, y_pred)
print("Confusion matrix:\n", conf_mat)

class_report = classification_report(y_test, y_pred)
print("Classification report:\n", class_report)

"""Un f1-score de 0 pour la classe 1 signifie que le modèle n'a pas été en mesure de prédire correctement la classe des observations positives (fraudes) dans l'ensemble de test. Cela peut être dû à différentes raisons telles que :

un déséquilibre important des classes dans les données
une absence de caractéristiques importantes pour distinguer les fraudes des transactions normales
une mauvaise performance du modèle ou des paramètres non-optimaux
Il est important de prendre en compte le contexte et la nature des données avant d'interpréter le résultat du f1-score. Dans le cas de la détection de fraude, le coût d'une fausse négative (prédire qu'une transaction est normale alors qu'elle est frauduleuse) peut être très élevé, donc il est important d'avoir un modèle capable de détecter un maximum de fraudes tout en ayant un taux de faux positifs acceptable. Il est possible que le modèle actuel ne soit pas capable de remplir cette fonction de manière satisfaisante, ce qui peut nécessiter l'exploration de différentes approches ou méthodes pour améliorer les résultats.

Dans le cas de la classification binaire, la précision (precision), le rappel (recall) et le score F1 (f1-score) sont des métriques importantes pour évaluer les performances du modèle.

La précision mesure la proportion de prédictions positives qui sont correctes par rapport à l'ensemble des prédictions positives. Ainsi, dans ce cas, la précision pour la classe 0 (transactions non frauduleuses) est de 1.0, ce qui signifie que toutes les prédictions positives pour la classe 0 sont correctes. Cependant, la précision pour la classe 1 (transactions frauduleuses) est de 0.0, ce qui signifie que le modèle n'a pas prédit correctement les transactions frauduleuses.

Le rappel mesure la proportion de vraies positives qui sont correctement identifiées parmi l'ensemble des vraies positives. Dans ce cas, le rappel pour la classe 0 est de 1.0, ce qui signifie que toutes les transactions non frauduleuses ont été correctement identifiées. Cependant, le rappel pour la classe 1 est de 0.0, ce qui signifie que le modèle n'a pas correctement identifié les transactions frauduleuses.

Le score F1 est une moyenne harmonique entre la précision et le rappel. Il donne une mesure pondérée de la performance globale du modèle. Dans ce cas, le score F1 pour la classe 0 est de 1.0, ce qui indique une bonne performance pour les transactions non frauduleuses. Cependant, le score F1 pour la classe 1 est également de 0.0, ce qui indique une mauvaise performance pour les transactions frauduleuses.

En résumé, le modèle semble être très performant pour les transactions non frauduleuses, mais ne parvient pas à détecter les transactions frauduleuses. Il est donc nécessaire de travailler sur l'amélioration de la détection des transactions frauduleuses.

***Solutions?***

Changer de modèle : il est possible que le modèle choisi ne soit pas adapté pour cette tâche. Il peut être utile d'essayer d'autres modèles plus performants ou mieux adaptés pour cette tâche.

Améliorer la qualité des données : il est possible que les données contiennent des erreurs ou du bruit qui affectent les performances du modèle. Il peut être utile de nettoyer et prétraiter les données pour améliorer leur qualité.

Il est important de noter que la résolution de ce problème peut être un processus itératif qui nécessite plusieurs tentatives et ajustements pour obtenir les résultats souhaités.
"""

